{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af67c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-pubsub\n",
      "  Downloading google_cloud_pubsub-2.33.0-py3-none-any.whl (321 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.3/321.3 KB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2\n",
      "  Downloading protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.2/323.2 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting opentelemetry-sdk>=1.27.0\n",
      "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opentelemetry-api>=1.27.0\n",
      "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0\n",
      "  Downloading google_api_core-2.28.1-py3-none-any.whl (173 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.7/173.7 KB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio-status>=1.33.2\n",
      "  Downloading grpcio_status-1.76.0-py3-none-any.whl (14 kB)\n",
      "Collecting grpcio<2.0.0,>=1.51.3\n",
      "  Downloading grpcio-1.76.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3.0.0,>=2.14.1\n",
      "  Downloading google_auth-2.43.0-py2.py3-none-any.whl (223 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.1/223.1 KB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpc-google-iam-v1<1.0.0,>=0.12.4\n",
      "  Downloading grpc_google_iam_v1-0.14.3-py3-none-any.whl (32 kB)\n",
      "Collecting proto-plus<2.0.0,>=1.22.0\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting googleapis-common-protos<2.0.0,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.5/297.5 KB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.18.0 in /home/kurek/GitHub/laborinsight-data/venv/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-cloud-pubsub) (2.32.5)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cachetools<7.0,>=2.0.0\n",
      "  Downloading cachetools-6.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /home/kurek/GitHub/laborinsight-data/venv/lib/python3.10/site-packages (from grpcio<2.0.0,>=1.51.3->google-cloud-pubsub) (4.15.0)\n",
      "Collecting importlib-metadata<8.8.0,>=6.0\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.59b0\n",
      "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 KB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting zipp>=3.20\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.1/83.1 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/kurek/GitHub/laborinsight-data/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-cloud-pubsub) (2025.10.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/kurek/GitHub/laborinsight-data/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-cloud-pubsub) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kurek/GitHub/laborinsight-data/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-cloud-pubsub) (2.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kurek/GitHub/laborinsight-data/venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.0->google-cloud-pubsub) (3.11)\n",
      "Installing collected packages: zipp, pyasn1, protobuf, grpcio, cachetools, rsa, pyasn1-modules, proto-plus, importlib-metadata, googleapis-common-protos, opentelemetry-api, grpcio-status, google-auth, opentelemetry-semantic-conventions, grpc-google-iam-v1, google-api-core, opentelemetry-sdk, google-cloud-pubsub\n",
      "Successfully installed cachetools-6.2.2 google-api-core-2.28.1 google-auth-2.43.0 google-cloud-pubsub-2.33.0 googleapis-common-protos-1.72.0 grpc-google-iam-v1-0.14.3 grpcio-1.76.0 grpcio-status-1.76.0 importlib-metadata-8.7.0 opentelemetry-api-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 proto-plus-1.26.1 protobuf-6.33.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 rsa-4.9.1 zipp-3.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install requests lxml beautifulsoup4 playwright google-cloud-pubsub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d059010",
   "metadata": {},
   "outputs": [],
   "source": [
    "!playwright install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c919386",
   "metadata": {},
   "outputs": [],
   "source": [
    "!playwright install-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782465c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import random\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "from google.cloud import pubsub_v1\n",
    "import hashlib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, BeautifulSoup as BS\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "SITEMAP_URL = \"https://static.theprotocol.it/sitemaps/CurrentOffers/SiteMapJobOffers1.xml\"\n",
    "CONCURRENCY = 5\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "DATA_FILENAME = f\"protocol_data_{timestamp}.jsonl\"\n",
    "LOG_FILENAME = f\"protocol_log_{timestamp}.log\"\n",
    "\n",
    "PROJECT_ID = \"laborinsight-data\"\n",
    "TOPIC_ID = \"jobs-raw\"\n",
    "publisher = pubsub_v1.PublisherClient()\n",
    "topic_path = publisher.topic_path(PROJECT_ID, TOPIC_ID)\n",
    "\n",
    "logger = logging.getLogger(\"protocol_scraper\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "file_handler = logging.FileHandler(LOG_FILENAME, encoding=\"utf-8\")\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "logger.handlers.clear()\n",
    "logger.addHandler(console_handler)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/129.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "def parse_list(section):\n",
    "    if not section:\n",
    "        return None\n",
    "    items = []\n",
    "    for li in section.select(\"li\"):\n",
    "        t = li.get_text(strip=True)\n",
    "        if t:\n",
    "            items.append(t)\n",
    "    return items if items else None\n",
    "\n",
    "\n",
    "def extract_title(soup: BeautifulSoup):\n",
    "    tag = soup.find(\"h1\")\n",
    "    return tag.get_text(strip=True) if tag else None\n",
    "\n",
    "\n",
    "def extract_company(soup: BeautifulSoup):\n",
    "    tag = soup.select_one('[data-test=\"text-offerEmployer\"]')\n",
    "    if not tag:\n",
    "        return None\n",
    "\n",
    "    span = tag.find(\"span\")\n",
    "    if span:\n",
    "        span.extract()\n",
    "\n",
    "    return tag.get_text(strip=True)\n",
    "\n",
    "def extract_salary(soup: BeautifulSoup):\n",
    "    container = soup.select_one('[data-test=\"text-salary-value\"]')\n",
    "    if not container:\n",
    "        return None\n",
    "\n",
    "    if container.get(\"data-is-secondary\") == \"true\":\n",
    "        return container.get_text(strip=True)\n",
    "\n",
    "    salary_value = container.select_one('[data-test=\"text-contractSalary\"]')\n",
    "    salary_units = container.select_one('[data-test=\"text-contractUnits\"]')\n",
    "    salary_time = container.select_one('[data-test=\"text-contractTimeUnits\"]')\n",
    "\n",
    "    parts = []\n",
    "    if salary_value:\n",
    "        parts.append(salary_value.get_text(\" \", strip=True))\n",
    "    if salary_units:\n",
    "        parts.append(salary_units.get_text(\" \", strip=True))\n",
    "    if salary_time:\n",
    "        parts.append(salary_time.get_text(\" \", strip=True))\n",
    "\n",
    "    return \" \".join(parts) if parts else None\n",
    "\n",
    "\n",
    "def extract_location(soup: BeautifulSoup):\n",
    "    loc = soup.select_one('[data-test=\"text-primaryLocation\"]')\n",
    "    if loc:\n",
    "        return loc.get_text(strip=True)\n",
    "\n",
    "    loc2 = soup.select_one('[data-test=\"text-currentLocation1\"]')\n",
    "    if loc2:\n",
    "        return loc2.get_text(strip=True)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_level(soup: BeautifulSoup):\n",
    "    tag = soup.select_one('[data-test=\"content-positionLevels\"]')\n",
    "    if not tag:\n",
    "        return None\n",
    "\n",
    "    txt = tag.get_text(strip=True).lower()\n",
    "    if \"/\" in txt:\n",
    "        return [x.strip() for x in txt.split(\"/\")]\n",
    "    return txt\n",
    "\n",
    "\n",
    "def extract_requirements(soup: BeautifulSoup):\n",
    "    result = {\"expected\": [], \"optional\": []}\n",
    "\n",
    "    expected = soup.select_one('[data-test=\"section-requirements-expected\"]')\n",
    "    optional = soup.select_one('[data-test=\"section-requirements-optional\"]')\n",
    "\n",
    "    if expected:\n",
    "        result[\"expected\"] = parse_list(expected) or []\n",
    "    if optional:\n",
    "        result[\"optional\"] = parse_list(optional) or []\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def extract_responsibilities(soup: BeautifulSoup):\n",
    "    section = soup.select_one('[data-test=\"section-responsibilities\"]')\n",
    "    return parse_list(section)\n",
    "\n",
    "\n",
    "def extract_offered_and_benefits(soup: BeautifulSoup):\n",
    "    offered = soup.select_one('[data-test=\"section-offered\"]')\n",
    "    benefits = soup.select_one('[data-test=\"section-benefits\"]')\n",
    "\n",
    "    return {\n",
    "        \"offered\": parse_list(offered),\n",
    "        \"benefits\": parse_list(benefits),\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_technologies(soup: BeautifulSoup):\n",
    "    sections = []\n",
    "\n",
    "    for h3 in soup.find_all(\"h3\"):\n",
    "        title = h3.get_text(strip=True).lower()\n",
    "        container = h3.find_next(\"div\")\n",
    "        if not container:\n",
    "            continue\n",
    "\n",
    "        chips = container.select('[data-test=\"chip-technology\"]')\n",
    "\n",
    "        values = []\n",
    "        for c in chips:\n",
    "            if c.get(\"title\"):\n",
    "                values.append(c[\"title\"].strip())\n",
    "            else:\n",
    "                t = c.get_text(strip=True)\n",
    "                if t:\n",
    "                    values.append(t)\n",
    "\n",
    "        if values:\n",
    "            sections.append(values)\n",
    "\n",
    "    required = sections[0] if len(sections) > 0 else []\n",
    "    nice = sections[1] if len(sections) > 1 else []\n",
    "\n",
    "    return required, nice\n",
    "\n",
    "\n",
    "def parse_offer(html: str, url: str):\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "    title = extract_title(soup)\n",
    "    company = extract_company(soup)\n",
    "    location = extract_location(soup)\n",
    "    level = extract_level(soup)\n",
    "    salary = extract_salary(soup)\n",
    "    tech_required, tech_nice = extract_technologies(soup)\n",
    "    requirements = extract_requirements(soup)\n",
    "    responsibilities = extract_responsibilities(soup)\n",
    "    offered = extract_offered_and_benefits(soup)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"company\": company,\n",
    "        \"location\": location,\n",
    "        \"level\": level,\n",
    "        \"salary\": salary,\n",
    "        \"tech_required\": tech_required,\n",
    "        \"tech_nice_to_have\": tech_nice,\n",
    "        \"requirements\": requirements,\n",
    "        \"responsibilities\": responsibilities,\n",
    "        \"offered\": offered,\n",
    "        \"url\": url,\n",
    "    }\n",
    "\n",
    "\n",
    "async def fetch_offer(page, url: str, retries: int = 3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            await asyncio.sleep(random.uniform(0.2, 1.1))\n",
    "            await page.goto(url, timeout=60000, wait_until=\"domcontentloaded\")\n",
    "            html = await page.content()\n",
    "            return parse_offer(html, url)\n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                logger.error(f\"Fetch error for {url}: {e}\")\n",
    "                return {\"url\": url, \"error\": str(e)}\n",
    "            await asyncio.sleep(1 + attempt)\n",
    "\n",
    "\n",
    "async def scrape_all(urls):\n",
    "    results = []\n",
    "\n",
    "    async with async_playwright() as pw:\n",
    "        browser = await pw.chromium.launch(\n",
    "            headless=True,\n",
    "            args=[\"--disable-blink-features=AutomationControlled\"],\n",
    "        )\n",
    "\n",
    "        semaphore = asyncio.Semaphore(CONCURRENCY)\n",
    "\n",
    "        async def worker(url, idx, total):\n",
    "            async with semaphore:\n",
    "                page = await browser.new_page(user_agent=HEADERS[\"User-Agent\"])\n",
    "                data = await fetch_offer(page, url)\n",
    "                await page.close()\n",
    "                logger.info(\n",
    "                    f\"[{idx}/{total}] {data.get('title')} | \"\n",
    "                    f\"{data.get('company')} | {data.get('location')} | {data.get('url')}\"\n",
    "                )\n",
    "\n",
    "                return data\n",
    "\n",
    "        tasks = [\n",
    "            asyncio.create_task(worker(url, i + 1, len(urls)))\n",
    "            for i, url in enumerate(urls)\n",
    "        ]\n",
    "\n",
    "        for task in asyncio.as_completed(tasks):\n",
    "            results.append(await task)\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "async def load_sitemap(url: str):\n",
    "    logger.info(\"Fetching sitemap (requests)...\")\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=30)\n",
    "        if r.status_code == 403:\n",
    "            logger.error(\"Got 403 Forbidden when fetching sitemap via requests. Falling back to Playwright...\")\n",
    "            raise requests.HTTPError(\"403 from requests\")\n",
    "        r.raise_for_status()\n",
    "        xml = BS(r.text, \"xml\")\n",
    "        urls = [loc.get_text() for loc in xml.find_all(\"loc\")]\n",
    "        logger.info(f\"Found {len(urls)} URLs in sitemap (requests)\")\n",
    "        return urls\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching sitemap via requests: {e}\")\n",
    "        logger.info(\"Trying to fetch sitemap via Playwright...\")\n",
    "\n",
    "    # Fallback: Playwright udająca przeglądarkę\n",
    "    try:\n",
    "        async with async_playwright() as pw:\n",
    "            browser = await pw.chromium.launch(headless=True)\n",
    "            page = await browser.new_page(user_agent=HEADERS[\"User-Agent\"])\n",
    "            await page.goto(url, wait_until=\"domcontentloaded\", timeout=60000)\n",
    "            xml_text = await page.content()\n",
    "            await browser.close()\n",
    "\n",
    "        # page.content() da Ci <html>...</html> z XML-em w środku, ale <loc> będą\n",
    "        soup = BS(xml_text, \"xml\")\n",
    "        urls = [loc.get_text() for loc in soup.find_all(\"loc\")]\n",
    "        logger.info(f\"Found {len(urls)} URLs in sitemap (Playwright)\")\n",
    "        return urls\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to fetch sitemap even via Playwright: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def make_fingerprint(offer: dict) -> str:\n",
    "    basis = f\"{offer.get('title','')}|{offer.get('company','')}|{offer.get('location','')}|{offer.get('url','')}\"\n",
    "    return hashlib.sha1(basis.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "async def main():\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    urls = await load_sitemap(SITEMAP_URL)\n",
    "    # urls = urls[:10]  # do testów\n",
    "\n",
    "    if not urls:\n",
    "        logger.error(\"No URLs fetched from sitemap. Exiting without scraping.\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Starting scraping of {len(urls)} offers...\")\n",
    "\n",
    "    results = await scrape_all(urls)\n",
    "\n",
    "    now = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    published = 0\n",
    "\n",
    "    for offer in results:\n",
    "        if not offer.get(\"title\") or \"error\" in offer:\n",
    "            continue \n",
    "\n",
    "        msg = {\n",
    "            \"source\": \"theprotocol\",\n",
    "            \"payload\": offer,\n",
    "            \"ingested_at\": now,\n",
    "            \"fingerprint\": make_fingerprint(offer),\n",
    "        }\n",
    "\n",
    "        data_bytes = json.dumps(msg, ensure_ascii=False).encode(\"utf-8\")\n",
    "        publisher.publish(topic_path, data=data_bytes)\n",
    "        published += 1\n",
    "\n",
    "    elapsed = time.perf_counter() - start\n",
    "    logger.info(f\"Published {published} messages to Pub/Sub topic {TOPIC_ID}\")\n",
    "    logger.info(f\"Exec time: {elapsed:.2f} s (~{elapsed/60:.1f} min)\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(main())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a70ddc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 13:39:47,347 [INFO] Fetching sitemap...\n",
      "2025-11-16 13:39:49,178 [INFO] Found 6112 URLs in sitemap\n",
      "2025-11-16 13:39:49,180 [INFO] Starting scraping of 10 offers...\n",
      "2025-11-16 13:39:52,941 [INFO] [4/10] Data analyst - product owner | Santander Bank Polska | Warszawa, Wola | https://theprotocol.it/szczegoly/praca/data-analyst---product-owner-warszawa-aleja-jana-pawla-ii-17,oferta,12a60000-fe33-2e51-c7b1-08de02813576\n",
      "2025-11-16 13:39:54,094 [INFO] [5/10] Data analyst | Santander Bank Polska | Warszawa, mazowieckie | https://theprotocol.it/szczegoly/praca/data-analyst-warszawa,oferta,08dd0000-54c1-3a03-1cc4-08de028153ef\n",
      "2025-11-16 13:39:54,512 [INFO] [2/10] Kurs na Kontrolera Ruchu Lotniczego | Polska Agencja Żeglugi Powietrznej | Warszawa, Włochy | https://theprotocol.it/szczegoly/praca/kurs-na-kontrolera-ruchu-lotniczego-warszawa-wiezowa-8,oferta,ed720000-f460-1a83-c9f2-08de01872a90\n",
      "2025-11-16 13:39:54,783 [INFO] [3/10] Senior Java Developer API Management (F/M) | PEOPLEVIBE SPÓŁKA Z OGRANICZONĄ ODPOWIEDZIALNOŚCIĄ | Warszawa, mazowieckie | https://theprotocol.it/szczegoly/praca/senior-java-developer-api-management-f-m-warszawa,oferta,08dd0000-54c1-3a03-6aea-08de0273ba7c\n",
      "2025-11-16 13:39:55,350 [INFO] [6/10] Data analyst | Santander Bank Polska | Wrocław, dolnośląskie | https://theprotocol.it/szczegoly/praca/data-analyst-wroclaw,oferta,24ad0000-c505-1a6b-7a1c-08de028153f4\n",
      "2025-11-16 13:39:55,528 [INFO] [1/10] Application / Product Security Engineer | ABB Business Services | Kraków, Stare Miasto | https://theprotocol.it/szczegoly/praca/application---product-security-engineer-krakow-starowislna-13a,oferta,67f80000-6c64-1ed5-374e-08ddf9ba53be\n",
      "2025-11-16 13:39:57,023 [INFO] [7/10] Data analyst | Santander Bank Polska | Poznań, wielkopolskie | https://theprotocol.it/szczegoly/praca/data-analyst-poznan,oferta,12a60000-fe33-2e51-b69d-08de028153f8\n",
      "2025-11-16 13:39:57,377 [INFO] [8/10] Network Administrator | Beyond.pl Sp. z o.o. | Poznań, Nowe Miasto | https://theprotocol.it/szczegoly/praca/network-administrator-poznan-adama-kreglewskiego-11,oferta,24ad0000-c505-1a6b-7d73-08de0282f4e8\n",
      "2025-11-16 13:39:57,380 [INFO] [9/10] Senior Node.js Developer | STARTUP DEVELOPMENT HOUSE sp. z o.o. | Warszawa, Masovian | https://theprotocol.it/szczegoly/praca/senior-nodejs-developer-warszawa-aleje-jerozolimskie-81,oferta,36180000-2fe9-1e51-a7b5-08de0585261d\n",
      "2025-11-16 13:39:57,658 [INFO] [10/10] Investment Compliance Specialist with SQL/ programming skills, Senior Associate | STATE STREET BANK INTERNATIONAL GMBH | Kraków, Dębniki | https://theprotocol.it/szczegoly/praca/investment-compliance-specialist-with-sql--programming-skills-senior-associate-krakow-generala-bohdana-zielinskiego-3,oferta,f5870000-9132-da00-a799-08de064b6fb2\n",
      "2025-11-16 13:39:57,759 [INFO] Published 10 messages to Pub/Sub topic jobs-raw\n",
      "2025-11-16 13:39:57,759 [INFO] Exec time: 10.41 s (~0.2 min)\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
